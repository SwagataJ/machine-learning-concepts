{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZMlRiJw5yI3hNRBQiMjS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwagataJ/machine-learning-concepts/blob/main/deep-learning/two_layer_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment : Week 1\n",
        "## Submission by: Swagata Jana\n",
        "\n",
        "#### Problem : Build a two-layer neural network from scratch In python.\n",
        "#### Description : To gain a better understanding of Deep Learning, build a Neural Network from scratch using only python, without a deep learning library like TensorFlow. Understanding the inner workings of a Neural Network is important to a ML Engineer. (You can use scientific libraries like numpy, scipy etc)\n",
        "#### Dataset: MNIST"
      ],
      "metadata": {
        "id": "F55OeLO0Jy_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the packages\n",
        "\n",
        "1.   Numpy: Numpy is used for scientific computations like dot product, exponents, etc\n",
        "2.   Matplotlib: Matplotlib is used for plotting the loss values against the epochs\n",
        "\n"
      ],
      "metadata": {
        "id": "vn0Ov5LHJ9k6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eEk81g0FIlhb"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset\n",
        "\n",
        "I am using **Keras** to import the dataset.\n",
        "Then, the data is split into training data and testing data respectively.\n",
        "\n",
        "PS: I tried downloading the dataset manually from [Yann Lecun](http://yann.lecun.com/exdb/mnist/) but I was not authorized, hence I was compelled to use a library."
      ],
      "metadata": {
        "id": "65M8vU8cLD2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the MNIST dataset from Keras datasets\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "KRRP_3LaI8Q0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "\n",
        "1. Flattening the image into a one dimensional vector of length 784.\n",
        "\n",
        "2. Normalizing the pixel values\n",
        "\n",
        "3. Converting output labels to numerical vectors( **one hot encoding** )"
      ],
      "metadata": {
        "id": "EIz0JLHxWCq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten images into a vector of 784 features\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = np.eye(num_classes)[y_train]\n",
        "y_test = np.eye(num_classes)[y_test]"
      ],
      "metadata": {
        "id": "UAOAXnvLa_xr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the hyperparameters\n",
        "\n",
        "Hyperparameters such as learning rate, epochs, number of nodes in each layers."
      ],
      "metadata": {
        "id": "P3DaykbkNCFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining hyperparameters\n",
        "inputLayer = 784\n",
        "outputLayer = 10\n",
        "hiddenLayer = 100\n",
        "learning_rate = 2\n",
        "n_epochs = 101"
      ],
      "metadata": {
        "id": "WkPMBN_jm85p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the activation functions"
      ],
      "metadata": {
        "id": "5TUyRSooaxxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))    \n",
        "\n",
        "# Derivative of Sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x)*(1 - sigmoid(x))\n",
        "\n",
        "# Softmax Funtion\n",
        "def softmax(x):\n",
        "    exps = np.exp(x)\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "P9o88jk-K2bb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Loss Function (**cross entropy loss**)"
      ],
      "metadata": {
        "id": "rCXGDaWxa5D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def crossEntropyLoss(y_pred, y_true):\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / x_train.shape[0]\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "cj-zUJDotQJy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward and Backward Propagation\n",
        "\n",
        "\n",
        "\n",
        "1. Forward propagation involves passing input through the network, computing activations by multiplying with weights, adding biases and applying activation functions. The output of one layer is the input to the next.\n",
        "\n",
        "2. Backward propagation is the process of computing gradients of the loss function with respect to the weights and biases of the neural network, using the chain rule of calculus. These gradients are used to update the weights and biases during training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eA2xZ5YzbD9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward and Backward Propagation\n",
        "def fowardAndBackwardPropagation(params, X, y, lr):\n",
        "    W1, b1, W2, b2 = params\n",
        "\n",
        "    Z1 = np.dot(X,W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2;\n",
        "    A2 = softmax(Z2)\n",
        "  \n",
        "    dz2 = (A2 - y)\n",
        "    dz1 = np.dot(dz2, W2.T) * sigmoid_derivative(Z1)\n",
        "\n",
        "    dw2 = np.dot(A1.T, dz2) / X.shape[0]\n",
        "    db2 = np.sum(dz2, axis=0) / X.shape[0]\n",
        "    dw1 = np.dot(X.T, dz1) / X.shape[0]\n",
        "    db1 = np.sum(dz1, axis=0) / X.shape[0]\n",
        "\n",
        "    loss = crossEntropyLoss(A2, y)\n",
        "\n",
        "    return dw1, db1, dw2, db2, loss"
      ],
      "metadata": {
        "id": "TYrXftDTCZE7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization of weights and biases\n",
        "\n",
        "1.   The weights have been initialized to abritrary random numbers and then multiplying by 0.01 so that the weights are small and close to zero.\n",
        "\n",
        "2.   The biases are all set to zero so that the network is neutral at the start.\n",
        "\n"
      ],
      "metadata": {
        "id": "CvLKLgtQbNPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the weights and biases\n",
        "W1 = np.random.randn(inputLayer, hiddenLayer) * 0.01\n",
        "b1 = np.zeros(hiddenLayer)\n",
        "W2 = np.random.randn(hiddenLayer, outputLayer) * 0.01\n",
        "b2 = np.zeros(outputLayer)"
      ],
      "metadata": {
        "id": "EzlillM3alWf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "Now the model is trained on the training data where its weights and baises are adjusted to minimise the loss and give a correct prediction for the output."
      ],
      "metadata": {
        "id": "Dd4CiY0hukxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "losses = list()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    params = (W1, b1, W2, b2)\n",
        "    dw1, db1, dw2, db2, loss = fowardAndBackwardPropagation(params, x_train, y_train, learning_rate)\n",
        "    W2 -= learning_rate * dw2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dw1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    losses.append(loss)\n",
        "    if epoch % 5 == 0:\n",
        "        print(f'Epoch {epoch}/{n_epochs} - loss: {loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37od3ZT_BD18",
        "outputId": "d9e9d7ec-b49a-4461-89ff-90e9a9a12406"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/101 - loss: 2.3034\n",
            "Epoch 5/101 - loss: 2.4095\n",
            "Epoch 10/101 - loss: 2.2423\n",
            "Epoch 15/101 - loss: 2.1220\n",
            "Epoch 20/101 - loss: 1.7977\n",
            "Epoch 25/101 - loss: 1.3760\n",
            "Epoch 30/101 - loss: 1.0687\n",
            "Epoch 35/101 - loss: 0.8751\n",
            "Epoch 40/101 - loss: 0.7872\n",
            "Epoch 45/101 - loss: 1.0465\n",
            "Epoch 50/101 - loss: 0.6177\n",
            "Epoch 55/101 - loss: 0.5794\n",
            "Epoch 60/101 - loss: 0.5854\n",
            "Epoch 65/101 - loss: 0.5310\n",
            "Epoch 70/101 - loss: 0.4709\n",
            "Epoch 75/101 - loss: 0.4575\n",
            "Epoch 80/101 - loss: 0.4456\n",
            "Epoch 85/101 - loss: 0.4190\n",
            "Epoch 90/101 - loss: 0.3924\n",
            "Epoch 95/101 - loss: 0.3738\n",
            "Epoch 100/101 - loss: 0.3614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The loss-epoch graph\n",
        "\n",
        "Here is the losses for each epoch is plotted in a graph using matplotlib."
      ],
      "metadata": {
        "id": "TADenM_cu9lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the loss\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "qRar9LSyA7ai",
        "outputId": "07ee79f5-4bc7-40ed-cab1-3709dbefdcc9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model\n",
        "\n",
        "Now the model is tested as to how the model performs on unseen data and its accuracy is printed."
      ],
      "metadata": {
        "id": "OcEpqSi1vLsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "Z1 = np.dot(x_test, W1) + b1\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.dot(A1, W2) + b2\n",
        "A2 = softmax(Z2)\n",
        "accuracy = np.mean(np.argmax(A2, axis=1) == np.argmax(y_test, axis=1))\n",
        "print(f\"Test accuracy: {accuracy*100}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5qy3qjHBxoo",
        "outputId": "107968e1-3437-4d11-bcda-787082e63b6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 90.16999999999999%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "The model achieves an accuracy of ***90.16%***. The accuracy can be further increased by:\n",
        "\n",
        "*  increasing the number number of hidden layers \n",
        "\n",
        "*  using an adaptive learning rate or tweaking the learning rate further\n",
        "\n",
        "*  increasing the number of neurons per layer\n",
        "\n",
        "*  using regularization / dropout, etc.\n",
        "\n",
        "\n",
        "## Brief summary of implementation\n",
        "\n",
        "\n",
        "1.   Load the MNIST dataset from Keras\n",
        "2.   Reshape the training data so that each sample is a flat vector of length 784.\n",
        "3.   Normalize the pixel values to be between 0 and 1\n",
        "4.   Convert the labels to one-hot encoding using the eye() function from NumPy.\n",
        "5.   Setting the hyperparameters, such as learning rate, number of nodes in a layer, the activation functions namely, sigmoid and its derivative and softmax.\n",
        "6.   Defining the loss function which will compute the loss for a particular prediction. I am using cross entropy loss.\n",
        "7. Computing the forward propagation and the backward propagation which uses the hyperparameters and the loss function.\n",
        "8. Initialising the weights and biases to arbitrary random values for weights and zeros for biases.\n",
        "9. Training the model, which uses the forward and backward propagation fucntion.\n",
        "10. Plotting the losses to check if our model is able to minimise the losses.\n",
        "11. Checking the accuracy\n",
        "\n",
        "## Problems faced on implementation\n",
        "\n",
        "\n",
        "*   I was not very familiar with numpy and had to look up individual functions that would be suitable for a particular operation\n",
        "*   I was not able to download the MNIST dataset from  [Yann Lecun](http://yann.lecun.com/exdb/mnist/) and then decided to move forward with importing the dataset from a library\n",
        "* Converting the mathematical formulae to code was a bit complex and I tried to simplify a lot of it on paper\n",
        "* In the losses graph the loss initially increases drastically, which alarmed me at first and made me think that I had made a huge error but realised that since the weights are generated randomly, it might not be suited to the problem at all.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9wpfLO4YwTkL"
      }
    }
  ]
}